---
title: "Predicting Weight-Lifting Quality"
author: "Aaron Ferrucci"
date: "October 17, 2015"
output: 
  html_document:
    fig_height: 4
    fig_width: 6
references:
- id: Ugulino2012
  title: "Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements"
  author:
  - family: Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H.
  container-title: Proceedings of 21st Brazilian Symposium on Artificial Intelligence
  URL: 'http://groupware.les.inf.puc-rio.br/har'
---


``` {r echo=FALSE, include=F, messages=F, warnings=F, results="hide"}
# Load libraries, data
set.seed(12345)
library(caret)
library(randomForest)
```

## Overview
TBD - this will be easy to write once I know what I'm doing.

## Preliminaries
The test and training data sets are provided as files in .csv format. First,
I'll load in the training data set and do some basic exploratory analysis.

``` {r echo=T}
train_all <- read.csv("pml-training.csv")
dim(train_all)
```

The data set consists of `r nrow(train_all)` observations of `r ncol(train_all)`
variables.  As explained in [@Ugulino2012], the observations are of 6
individuals performing a weight-lifting exercise correctly, and then
incorrectly in a number of ways. Correctness or not is encoded in the
`classe` variable of the dataset, which is a factor variable of 5 levels, with
the following interpretation:

* A: exactly according to the specification
* B: throwing the elbows to the front
* C: lifting the dumbbell only halfway
* D: lowering the dumbbell only halfway
* E: throwing the hips to the front 

A contingency table by subject and class shows how many of each type of measurement were made, for each individual:
``` {r}
table(train_all$classe, train_all$user_name)
```

## Reducing Covariates
To improve scalablity of any classification algorithms, it's worth trying to
reduce the data size to only those measurements which matter. 

Looking over the measurements, it's clear that a large number of the
measurements are NAs or empty string (""):

``` {r}
nas <- apply(train_all, 2, function(x) sum(is.na(x) || x == ""))
t <- table(nas)
t
```

Noting that when a measurement contains any missing values, it contains a 
large number of them (`r t[2]` out of `r nrow(train_all)` total measurements).
Looking closer, I
see that the missing measurements attain values when the variable
`r "new_window"` is "yes"; the names of the missing measurements indicate that
these are summary statistics (min, max, avg, etc.) - in other words, they are
transformed tidy covariates or Level 2 covariates. In the interest of letting
a machine learning algorithm discover the necessary variables, I will discard
all transformed covariates.

``` {r}
train_all <- train_all[, nas == 0]
```

Next, we can eliminate more less-useful covariates via `nearZeroVar`:

``` {r}
nzv <- nearZeroVar(train_all, saveMetrics=T)
names(train_all)[nzv$nzv]
train_all <- train_all[,!nzv$nzv]
```

Finally, we can discard a few covariates based on "common sense". For example,
the `r "X"` value is a simple index over all of the data; there are also a 
few timestamp values that are unlikely to be predictors for correct vs. 
incorrect weightlifting technique.

``` {r}
train_all <- train_all[, -grep("timestamp|X|num_window", names(train_all))]
```

I've reduced the number of measurements to `r ncol(train_all)`, which is a big
improvement over the original data size.

## Preparing for Cross-Validation
I will separate the training set into train and validation subsets, and use 
the validation subset error as my out-of-sample error estimate.
``` {r}
dp <- createDataPartition(y=train_all$classe, p=0.75, list=F) 
train <- train_all[dp,]
validate <- train_all[-dp,]
```

## Random Forest
A random forest is trained on the separated training data. Contrary to some
rumors, creation of the model takes only a few minutes, even on my 
ancient laptop.
``` {r cache=TRUE}
rf <- randomForest(classe ~ ., data=train)
rf
```

### Examining the Model: Variable Importance
Now that the model is trained, it's interesting to have a look at variable
importance. Can we trim this model, for better scalability, without sacrificing
accuracy?
``` {r}
order <- order(-rf$importance)
plot(rf$importance[order])
sorted_importance <-
  data.frame(
    var = rownames(rf$importance)[order],
    importance = rf$importance[order]
  )
head(sorted_importance, n=10)
```

The plot of variable importance looks rather promising - it's likely that 10 or
15 of the "least important" variables could be trimmed without losing a lot of
accuracy.

### Examining the Model: Estimate Out-of-Sample Error
I'll use the reserved validation data to estimate the out-of-sample data. 
The concept here is that since we don't have access to the test data, we
estimate the error on the test set using training data which was separated
out before building the model.

How does the model perform on the validation data?
``` {r}
modelError <- function(fit, data) {
  missClass <- sum(predict(fit, data) == data$classe)
  err <- missClass / nrow(data)
  return(err * 100)
}

modelError(rf, validate)
```

``` {r eval=FALSE, echo=FALSE}
# ``` {r eval=T, echo=T}
test <- read.csv("pml-testing.csv")
answers <- predict(rf, test)
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n) {
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
answers
pml_write_files(answers)
```
## References

